{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a4dd39",
   "metadata": {},
   "source": [
    "# Analisi e Valutazione Modello CNN\n",
    "\n",
    "In questo notebook analizziamo in dettaglio le performance del modello CNN addestrato per la classificazione DeepWeeds.\n",
    "\n",
    "## Obiettivi:\n",
    "- Valutare le performance sui dati di test\n",
    "- Analizzare le curve di training e overfitting\n",
    "- Generare confusion matrix e classification report\n",
    "- Analisi degli errori e confidence scores\n",
    "- Visualizzazioni avanzate delle performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3432e502",
   "metadata": {},
   "source": [
    "# 1. Setup e Caricamento Modello\n",
    "\n",
    "Importiamo le librerie e carichiamo il modello addestrato più recente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7871f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Matplotlib version: 3.10.3\n"
     ]
    }
   ],
   "source": [
    "# Import essenziali\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# Configurazione grafica\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPATIBILITÀ GOOGLE COLAB\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Rileva se siamo su Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"🔍 Ambiente rilevato: Google Colab\")\n",
    "    \n",
    "    # Monta Drive e configura path\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    PROJECT_PATH = '/content/drive/MyDrive/Progetto-Deep-Learning'\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    sys.path.append(f\"{PROJECT_PATH}/src\")\n",
    "    \n",
    "    MODELS_DIR = f\"{PROJECT_PATH}/models\"\n",
    "    DATA_DIR = f\"{PROJECT_PATH}/data\"\n",
    "    \n",
    "    print(f\"📁 Progetto configurato: {PROJECT_PATH}\")\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"🔍 Ambiente rilevato: Locale\")\n",
    "    \n",
    "    PROJECT_PATH = str(Path.cwd().parent)\n",
    "    MODELS_DIR = f\"{PROJECT_PATH}/models\"\n",
    "    DATA_DIR = f\"{PROJECT_PATH}/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b925ad",
   "metadata": {},
   "source": [
    "# 2. Caricamento Dataset e Modello\n",
    "\n",
    "Carichiamo i dataset preprocessati e il modello addestrato più recente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03937628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CARICAMENTO DATASET E MODELLO ===\n",
      "✅ Caricamento dataset preprocessati...\n",
      "WARNING:tensorflow:From C:\\Users\\amema\\AppData\\Local\\Temp\\ipykernel_17356\\671604865.py:11: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.load(...)` instead.\n",
      "   Dataset caricati: 9 classi, shape (256, 256, 3)\n",
      "📥 Caricamento modello da: ../models\\cnn_from_scratch_20250729_174718\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Modello non trovato in: ../models\\cnn_from_scratch_20250729_174718/checkpoints/best_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Modello caricato con successo!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModello non trovato in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Carica informazioni training se disponibili\u001b[39;00m\n\u001b[32m     55\u001b[39m training_info_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_model_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/training_info.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Modello non trovato in: ../models\\cnn_from_scratch_20250729_174718/checkpoints/best_model.h5"
     ]
    }
   ],
   "source": [
    "print(\"=== CARICAMENTO DATASET E MODELLO ===\")\n",
    "\n",
    "# Carica dataset preprocessati\n",
    "# Usa path dinamici se configurati (compatibilità Colab)  \n",
    "if 'DATA_DIR' in globals():\n",
    "    processed_dir = f\"{DATA_DIR}/processed\"\n",
    "else:\n",
    "    processed_dir = \"../data/processed\"\n",
    "\n",
    "if (os.path.exists(f\"{processed_dir}/train_dataset\") and \n",
    "    os.path.exists(f\"{processed_dir}/val_dataset\") and \n",
    "    os.path.exists(f\"{processed_dir}/test_dataset\")):\n",
    "    \n",
    "    print(\"✅ Caricamento dataset preprocessati...\")\n",
    "    train_dataset = tf.data.experimental.load(f\"{processed_dir}/train_dataset\")\n",
    "    val_dataset = tf.data.experimental.load(f\"{processed_dir}/val_dataset\")\n",
    "    test_dataset = tf.data.experimental.load(f\"{processed_dir}/test_dataset\")\n",
    "    \n",
    "    # Estrai informazioni dal dataset\n",
    "    sample_batch = train_dataset.take(1)\n",
    "    for images, labels in sample_batch:\n",
    "        INPUT_SHAPE = images.shape[1:]\n",
    "        \n",
    "        # Calcola numero di classi\n",
    "        all_labels = set()\n",
    "        for dataset in [train_dataset, val_dataset, test_dataset]:\n",
    "            for _, batch_labels in dataset:\n",
    "                all_labels.update(batch_labels.numpy())\n",
    "        \n",
    "        num_classes = len(all_labels)\n",
    "        class_names = [f\"Class_{i}\" for i in range(num_classes)]\n",
    "        break\n",
    "        \n",
    "    print(f\"   Dataset caricati: {num_classes} classi, shape {INPUT_SHAPE}\")\n",
    "    \n",
    "else:\n",
    "    raise FileNotFoundError(\"Dataset preprocessati non trovati! Esegui prima 03-preprocessing.ipynb\")\n",
    "\n",
    "# Trova il modello più recente\n",
    "# Usa path dinamici se configurati (compatibilità Colab)\n",
    "if 'MODELS_DIR' in globals():\n",
    "    models_dir = MODELS_DIR\n",
    "else:\n",
    "    models_dir = \"../models\"\n",
    "\n",
    "model_dirs = glob.glob(f\"{models_dir}/cnn_from_scratch_*\")\n",
    "\n",
    "if not model_dirs:\n",
    "    raise FileNotFoundError(\"Nessun modello CNN trovato! Esegui prima 04-cnn-from-scratch.ipynb\")\n",
    "\n",
    "# Ordina per data di creazione (più recente)\n",
    "latest_model_dir = max(model_dirs, key=os.path.getctime)\n",
    "model_path = f\"{latest_model_dir}/checkpoints/best_model.h5\"\n",
    "\n",
    "print(f\"📥 Caricamento modello da: {latest_model_dir}\")\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\"✅ Modello caricato con successo!\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Modello non trovato in: {model_path}\")\n",
    "\n",
    "# Carica informazioni training se disponibili\n",
    "training_info_path = f\"{latest_model_dir}/training_info.json\"\n",
    "training_log_path = f\"{latest_model_dir}/training_log.csv\"\n",
    "\n",
    "if os.path.exists(training_info_path):\n",
    "    with open(training_info_path, 'r') as f:\n",
    "        training_info = json.load(f)\n",
    "    print(f\"   Training completato in: {training_info.get('duration_seconds', 0):.0f} secondi\")\n",
    "    print(f\"   Epoch totali: {training_info.get('total_epochs', 'N/A')}\")\n",
    "    print(f\"   Migliore val_accuracy: {training_info.get('best_val_accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "if os.path.exists(training_log_path):\n",
    "    training_history = pd.read_csv(training_log_path)\n",
    "    print(f\"   Log training disponibile: {len(training_history)} epoch\")\n",
    "else:\n",
    "    training_history = None\n",
    "    print(\"   Log training non disponibile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e0770",
   "metadata": {},
   "source": [
    "# 3. Analisi Curve di Training\n",
    "\n",
    "Analizziamo l'andamento del training attraverso le curve di loss e accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_history is not None:\n",
    "    print(\"=== ANALISI CURVE DI TRAINING ===\")\n",
    "    \n",
    "    # Estrazione dati\n",
    "    epochs_range = range(1, len(training_history) + 1)\n",
    "    train_accuracy = training_history['accuracy']\n",
    "    val_accuracy = training_history['val_accuracy']\n",
    "    train_loss = training_history['loss']\n",
    "    val_loss = training_history['val_loss']\n",
    "    \n",
    "    # Verifica se esiste top3_accuracy\n",
    "    has_top3 = 'top3_accuracy' in training_history.columns\n",
    "    if has_top3:\n",
    "        train_top3 = training_history['top3_accuracy']\n",
    "        val_top3 = training_history['val_top3_accuracy']\n",
    "    \n",
    "    # Creazione subplot per analisi completa\n",
    "    fig_size = (16, 12) if has_top3 else (12, 8)\n",
    "    n_plots = 4 if has_top3 else 3\n",
    "    fig, axes = plt.subplots(2, 2, figsize=fig_size)\n",
    "    \n",
    "    # 1. Accuracy Curves\n",
    "    axes[0, 0].plot(epochs_range, train_accuracy, 'b-', label='Training Accuracy', linewidth=2)\n",
    "    axes[0, 0].plot(epochs_range, val_accuracy, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_ylim([0, 1])\n",
    "    \n",
    "    # 2. Loss Curves\n",
    "    axes[0, 1].plot(epochs_range, train_loss, 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 1].plot(epochs_range, val_loss, 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Overfitting Analysis\n",
    "    accuracy_gap = train_accuracy - val_accuracy\n",
    "    axes[1, 0].plot(epochs_range, accuracy_gap, 'red', linewidth=2, label='Train - Val Accuracy')\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1, 0].axhline(y=0.1, color='orange', linestyle='--', alpha=0.5, label='Overfitting Threshold')\n",
    "    axes[1, 0].set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy Gap')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Top-3 Accuracy o Learning Rate\n",
    "    if has_top3:\n",
    "        axes[1, 1].plot(epochs_range, train_top3, 'g-', label='Training Top-3', linewidth=2)\n",
    "        axes[1, 1].plot(epochs_range, val_top3, 'orange', label='Validation Top-3', linewidth=2)\n",
    "        axes[1, 1].set_title('Top-3 Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Top-3 Accuracy')\n",
    "        axes[1, 1].set_ylim([0, 1])\n",
    "    else:\n",
    "        # Smooth delle curve per trend analysis\n",
    "        window = max(1, len(train_accuracy) // 10)\n",
    "        smooth_train_acc = train_accuracy.rolling(window=window, center=True).mean()\n",
    "        smooth_val_acc = val_accuracy.rolling(window=window, center=True).mean()\n",
    "        \n",
    "        axes[1, 1].plot(epochs_range, smooth_train_acc, 'b-', label='Smooth Train Acc', linewidth=2)\n",
    "        axes[1, 1].plot(epochs_range, smooth_val_acc, 'r-', label='Smooth Val Acc', linewidth=2)\n",
    "        axes[1, 1].set_title('Smoothed Accuracy Trends', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Smoothed Accuracy')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{latest_model_dir}/detailed_training_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiche training\n",
    "    final_train_acc = train_accuracy.iloc[-1]\n",
    "    final_val_acc = val_accuracy.iloc[-1]\n",
    "    best_val_acc = val_accuracy.max()\n",
    "    best_val_epoch = val_accuracy.idxmax() + 1\n",
    "    final_overfitting_gap = final_train_acc - final_val_acc\n",
    "    \n",
    "    print(f\"📈 STATISTICHE TRAINING:\")\n",
    "    print(f\"   Accuracy finale - Train: {final_train_acc:.4f}, Val: {final_val_acc:.4f}\")\n",
    "    print(f\"   Migliore Val Accuracy: {best_val_acc:.4f} (epoch {best_val_epoch})\")\n",
    "    print(f\"   Gap finale Train-Val: {final_overfitting_gap:.4f}\")\n",
    "    \n",
    "    if final_overfitting_gap > 0.1:\n",
    "        print(f\"⚠️  Possibile overfitting (gap > 0.1)\")\n",
    "    else:\n",
    "        print(f\"✅ Overfitting sotto controllo\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ Storia del training non disponibile - saltando analisi curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef497945",
   "metadata": {},
   "source": [
    "# 4. Valutazione sul Test Set\n",
    "\n",
    "Valutiamo le performance finali del modello sui dati di test mai visti durante il training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== VALUTAZIONE SUL TEST SET ===\")\n",
    "\n",
    "# Valutazione quantitativa\n",
    "print(\"🔍 Valutazione in corso...\")\n",
    "test_results = model.evaluate(test_dataset, verbose=1)\n",
    "\n",
    "# Estrai metriche (order depends on model compilation)\n",
    "test_loss = test_results[0]\n",
    "test_accuracy = test_results[1]\n",
    "if len(test_results) > 2:\n",
    "    test_top3_accuracy = test_results[2]\n",
    "else:\n",
    "    test_top3_accuracy = None\n",
    "\n",
    "print(f\"\\n📊 RISULTATI FINALI SUL TEST SET:\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "if test_top3_accuracy:\n",
    "    print(f\"   Test Top-3 Accuracy: {test_top3_accuracy:.4f} ({test_top3_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Generazione predizioni per analisi dettagliata\n",
    "print(\"\\n🎯 Generazione predizioni dettagliate...\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "for batch_images, batch_labels in test_dataset:\n",
    "    predictions = model.predict(batch_images, verbose=0)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    y_true.extend(batch_labels.numpy())\n",
    "    y_pred.extend(predicted_classes)\n",
    "    y_pred_proba.extend(predictions)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(f\"✅ Analisi completata su {len(y_true)} campioni di test\")\n",
    "\n",
    "# Calcolo accuracy manuale per verifica\n",
    "manual_accuracy = np.mean(y_true == y_pred)\n",
    "print(f\"   Verifica accuracy: {manual_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd793b9b",
   "metadata": {},
   "source": [
    "# 5. Classification Report e Confusion Matrix\n",
    "\n",
    "Generiamo analisi dettagliate per classe delle performance del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CLASSIFICATION REPORT ===\")\n",
    "class_report = classification_report(\n",
    "    y_true, y_pred, \n",
    "    target_names=class_names,\n",
    "    digits=4\n",
    ")\n",
    "print(class_report)\n",
    "\n",
    "# Salva il report\n",
    "with open(f\"{latest_model_dir}/classification_report.txt\", 'w') as f:\n",
    "    f.write(class_report)\n",
    "\n",
    "print(\"\\n=== CONFUSION MATRIX ===\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Visualizzazione della confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Numero di Campioni'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predetto', fontsize=12)\n",
    "plt.ylabel('Reale', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{latest_model_dir}/confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Accuracy per classe\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "class_support = cm.sum(axis=1)\n",
    "\n",
    "class_df = pd.DataFrame({\n",
    "    'Classe': class_names,\n",
    "    'Accuracy': class_accuracies,\n",
    "    'Support': class_support,\n",
    "    'Correct': cm.diagonal(),\n",
    "    'Total': class_support\n",
    "}).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n📊 PERFORMANCE PER CLASSE:\")\n",
    "print(class_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Visualizzazione accuracy per classe\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(range(len(class_names)), class_accuracies, \n",
    "               color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Classi', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Accuracy per Classe sul Test Set', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(len(class_names)), class_names, rotation=45, ha='right')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Aggiungi valori e support sulle barre\n",
    "for i, (bar, acc, support) in enumerate(zip(bars, class_accuracies, class_support)):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{acc:.3f}\\n({support})', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{latest_model_dir}/accuracy_per_class.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ef1ad",
   "metadata": {},
   "source": [
    "# 6. Analisi degli Errori e Confidence\n",
    "\n",
    "Analizziamo gli errori del modello e la distribuzione dei confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ANALISI CONFIDENCE E ERRORI ===\")\n",
    "\n",
    "# Confidence analysis\n",
    "max_confidences = np.max(y_pred_proba, axis=1)\n",
    "correct_predictions = (y_true == y_pred)\n",
    "\n",
    "# Top-3 analysis se disponibile\n",
    "if y_pred_proba.shape[1] >= 3:\n",
    "    top3_predictions = np.argsort(y_pred_proba, axis=1)[:, -3:]\n",
    "    top3_correct = np.array([y_true[i] in top3_predictions[i] for i in range(len(y_true))])\n",
    "    top3_accuracy_manual = np.mean(top3_correct)\n",
    "    print(f\"Top-3 Accuracy (calcolata): {top3_accuracy_manual:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Distribuzione confidence\n",
    "axes[0, 0].hist(max_confidences[correct_predictions], bins=30, alpha=0.7, \n",
    "                label='Predizioni Corrette', color='green', density=True)\n",
    "axes[0, 0].hist(max_confidences[~correct_predictions], bins=30, alpha=0.7, \n",
    "                label='Predizioni Errate', color='red', density=True)\n",
    "axes[0, 0].set_xlabel('Confidence Score')\n",
    "axes[0, 0].set_ylabel('Densità')\n",
    "axes[0, 0].set_title('Distribuzione Confidence Scores')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Accuracy vs Confidence bins\n",
    "confidence_bins = np.linspace(0, 1, 11)\n",
    "bin_accuracies = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(len(confidence_bins)-1):\n",
    "    mask = (max_confidences >= confidence_bins[i]) & (max_confidences < confidence_bins[i+1])\n",
    "    if np.sum(mask) > 0:\n",
    "        bin_accuracy = np.mean(correct_predictions[mask])\n",
    "        bin_count = np.sum(mask)\n",
    "    else:\n",
    "        bin_accuracy = 0\n",
    "        bin_count = 0\n",
    "    bin_accuracies.append(bin_accuracy)\n",
    "    bin_counts.append(bin_count)\n",
    "\n",
    "bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "bars = axes[0, 1].bar(bin_centers, bin_accuracies, width=0.08, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('Confidence Range')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Accuracy vs Confidence')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Aggiungi count labels\n",
    "for bar, count in zip(bars, bin_counts):\n",
    "    if count > 0:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                        f'{count}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 3. Errori per confidence threshold\n",
    "thresholds = np.linspace(0.5, 1.0, 11)\n",
    "accuracies_at_threshold = []\n",
    "coverage_at_threshold = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    mask = max_confidences >= threshold\n",
    "    if np.sum(mask) > 0:\n",
    "        acc = np.mean(correct_predictions[mask])\n",
    "        coverage = np.sum(mask) / len(mask)\n",
    "    else:\n",
    "        acc = 0\n",
    "        coverage = 0\n",
    "    accuracies_at_threshold.append(acc)\n",
    "    coverage_at_threshold.append(coverage)\n",
    "\n",
    "axes[1, 0].plot(thresholds, accuracies_at_threshold, 'b-o', label='Accuracy', linewidth=2)\n",
    "axes[1, 0].plot(thresholds, coverage_at_threshold, 'r-s', label='Coverage', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Confidence Threshold')\n",
    "axes[1, 0].set_ylabel('Metric Value')\n",
    "axes[1, 0].set_title('Accuracy-Coverage Trade-off')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "\n",
    "# 4. Matrice delle classi più confuse\n",
    "# Trova le coppie di classi più confuse (errori off-diagonal)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "np.fill_diagonal(cm_normalized, 0)  # Rimuovi diagonale\n",
    "\n",
    "# Top 10 errori più frequenti\n",
    "confused_pairs = []\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confused_pairs.append((i, j, cm[i, j], cm_normalized[i, j]))\n",
    "\n",
    "confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "top_confusions = confused_pairs[:10]\n",
    "\n",
    "if top_confusions:\n",
    "    confusion_data = []\n",
    "    for true_idx, pred_idx, count, rate in top_confusions:\n",
    "        confusion_data.append({\n",
    "            'True Class': class_names[true_idx],\n",
    "            'Predicted Class': class_names[pred_idx],\n",
    "            'Count': count,\n",
    "            'Error Rate': rate\n",
    "        })\n",
    "    \n",
    "    confusion_df = pd.DataFrame(confusion_data)\n",
    "    \n",
    "    # Grafico a barre delle confusioni principali\n",
    "    y_pos = np.arange(len(confusion_df))\n",
    "    axes[1, 1].barh(y_pos, confusion_df['Count'], alpha=0.7, color='salmon')\n",
    "    axes[1, 1].set_yticks(y_pos)\n",
    "    axes[1, 1].set_yticklabels([f\"{row['True Class']} → {row['Predicted Class']}\" \n",
    "                                for _, row in confusion_df.iterrows()], fontsize=8)\n",
    "    axes[1, 1].set_xlabel('Numero di Errori')\n",
    "    axes[1, 1].set_title('Top 10 Confusioni tra Classi')\n",
    "    axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'Nessuna confusione\\nsignificativa rilevata', \n",
    "                    ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n",
    "    axes[1, 1].set_title('Analisi Confusioni')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{latest_model_dir}/error_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistiche confidence\n",
    "avg_confidence_correct = np.mean(max_confidences[correct_predictions])\n",
    "avg_confidence_wrong = np.mean(max_confidences[~correct_predictions])\n",
    "\n",
    "print(f\"\\n📈 STATISTICHE CONFIDENCE:\")\n",
    "print(f\"   Confidence media (corrette): {avg_confidence_correct:.4f}\")\n",
    "print(f\"   Confidence media (errate): {avg_confidence_wrong:.4f}\")\n",
    "print(f\"   Differenza: {avg_confidence_correct - avg_confidence_wrong:.4f}\")\n",
    "\n",
    "if top_confusions:\n",
    "    print(f\"\\n🎯 TOP 5 CONFUSIONI:\")\n",
    "    for i, (true_idx, pred_idx, count, rate) in enumerate(top_confusions[:5]):\n",
    "        print(f\"   {i+1}. {class_names[true_idx]} → {class_names[pred_idx]}: \"\n",
    "              f\"{count} errori ({rate:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cf01e",
   "metadata": {},
   "source": [
    "# 7. Riepilogo Finale e Salvataggio Risultati\n",
    "\n",
    "Generiamo un riepilogo completo delle performance e salviamo tutti i risultati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RIEPILOGO FINALE ===\")\n",
    "\n",
    "# Raccolta di tutte le metriche\n",
    "final_results = {\n",
    "    'model_path': latest_model_dir,\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'test_samples': int(len(y_true)),\n",
    "    'num_classes': int(num_classes),\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_loss': float(test_loss),\n",
    "    'avg_confidence_correct': float(avg_confidence_correct),\n",
    "    'avg_confidence_wrong': float(avg_confidence_wrong),\n",
    "    'confidence_gap': float(avg_confidence_correct - avg_confidence_wrong),\n",
    "}\n",
    "\n",
    "if test_top3_accuracy:\n",
    "    final_results['test_top3_accuracy'] = float(test_top3_accuracy)\n",
    "\n",
    "if training_history is not None:\n",
    "    final_results.update({\n",
    "        'final_train_accuracy': float(final_train_acc),\n",
    "        'final_val_accuracy': float(final_val_acc),\n",
    "        'best_val_accuracy': float(best_val_acc),\n",
    "        'best_val_epoch': int(best_val_epoch),\n",
    "        'overfitting_gap': float(final_overfitting_gap),\n",
    "        'total_training_epochs': len(training_history)\n",
    "    })\n",
    "\n",
    "# Performance per classe\n",
    "class_metrics = {}\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_metrics[class_name] = {\n",
    "        'accuracy': float(class_accuracies[i]),\n",
    "        'support': int(class_support[i]),\n",
    "        'correct_predictions': int(cm.diagonal()[i])\n",
    "    }\n",
    "\n",
    "final_results['class_performance'] = class_metrics\n",
    "\n",
    "# Salva risultati completi\n",
    "results_path = f\"{latest_model_dir}/complete_evaluation_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "\n",
    "# Crea summary testuale\n",
    "summary_text = f\"\"\"\n",
    "RIEPILOGO VALUTAZIONE MODELLO CNN\n",
    "================================\n",
    "\n",
    "Data Valutazione: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Modello: {latest_model_dir}\n",
    "\n",
    "PERFORMANCE TEST SET:\n",
    "- Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\n",
    "- Loss: {test_loss:.4f}\n",
    "- Campioni testati: {len(y_true):,}\n",
    "\"\"\"\n",
    "\n",
    "if test_top3_accuracy:\n",
    "    summary_text += f\"- Top-3 Accuracy: {test_top3_accuracy:.4f} ({test_top3_accuracy*100:.2f}%)\\n\"\n",
    "\n",
    "if training_history is not None:\n",
    "    summary_text += f\"\"\"\n",
    "TRAINING INFO:\n",
    "- Epoch totali: {len(training_history)}\n",
    "- Migliore Val Accuracy: {best_val_acc:.4f} (epoch {best_val_epoch})\n",
    "- Overfitting gap finale: {final_overfitting_gap:.4f}\n",
    "\"\"\"\n",
    "\n",
    "summary_text += f\"\"\"\n",
    "CONFIDENCE ANALYSIS:\n",
    "- Confidence media (corrette): {avg_confidence_correct:.4f}\n",
    "- Confidence media (errate): {avg_confidence_wrong:.4f}\n",
    "- Gap confidence: {avg_confidence_correct - avg_confidence_wrong:.4f}\n",
    "\n",
    "PERFORMANCE PER CLASSE:\n",
    "\"\"\"\n",
    "\n",
    "for class_name, acc in zip(class_names, class_accuracies):\n",
    "    summary_text += f\"- {class_name}: {acc:.4f}\\n\"\n",
    "\n",
    "summary_text += f\"\"\"\n",
    "FILES GENERATI:\n",
    "- Risultati completi: complete_evaluation_results.json\n",
    "- Classification report: classification_report.txt\n",
    "- Confusion matrix: confusion_matrix.png\n",
    "- Analisi training: detailed_training_analysis.png\n",
    "- Analisi errori: error_analysis.png\n",
    "- Accuracy per classe: accuracy_per_class.png\n",
    "\"\"\"\n",
    "\n",
    "# Salva summary\n",
    "summary_path = f\"{latest_model_dir}/evaluation_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(summary_text)\n",
    "\n",
    "print(f\"\\n✅ VALUTAZIONE COMPLETATA!\")\n",
    "print(f\"📁 Tutti i file salvati in: {latest_model_dir}\")\n",
    "print(f\"📊 Risultati principali:\")\n",
    "print(f\"   - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"   - Confidence Gap: {avg_confidence_correct - avg_confidence_wrong:.4f}\")\n",
    "if training_history is not None:\n",
    "    print(f\"   - Overfitting: {'⚠️ Presente' if final_overfitting_gap > 0.1 else '✅ Controllato'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
